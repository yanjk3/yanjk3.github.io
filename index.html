<!DOCTYPE HTML>
<html lang="zh-CN">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>
			Junkai Yan
		</title>
		<meta name="author" content="Junkai Yan">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="stylesheet.css">
		<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸŒ</text></svg>"></head>
	<body>
		<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tbody>
				<tr style="padding:0px">
					<td style="padding:0px">
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr style="padding:0px">
									<td style="padding:2.5%;width:63%;vertical-align:middle">
										<p class="name" style="text-align: center;">
											ä¸¥ä¿Šæ¥· (Drinky)
										</p>
										<p>
											<!-- I obtained a Master's degree in Computer Technology from the School
											of Computer Science and Engineering at <a href="https://www.sysu.edu.cn/">Sun Yat-sen University (SYSU)</a> in June 2024, where I am advised by <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Prof. Wei-Shi Zheng</a>. Before achieving my Master degree, I also obtained my B.E. degree at SYSU. -->
											æˆ‘äº2024å¹´6æœˆæ¯•ä¸šäº
											<a href="https://www.sysu.edu.cn/">
												ä¸­å±±å¤§å­¦
											</a>
											è®¡ç®—æœºå­¦é™¢ï¼Œè·å¾—å·¥å­¦ç¡•å£«å­¦ä½ï¼Œä¸“ä¸šä¸ºè®¡ç®—æœºæŠ€æœ¯ã€‚æˆ‘çš„å¯¼å¸ˆæ˜¯
											<a href="https://www.isee-ai.cn/~zhwshi/">
												éƒ‘ä¼Ÿè¯—æ•™æˆ
											</a>
											ã€‚åœ¨æˆ‘æ”»è¯»ç¡•å£«å­¦ä½ä¹‹å‰ï¼Œæˆ‘ä¹Ÿåœ¨ä¸­å±±å¤§å­¦è®¡ç®—æœºå­¦é™¢è·å¾—äº†å­¦å£«å­¦ä½ã€‚æˆ‘å¾ˆæ„Ÿæ©åœ¨ä¸­å±±å¤§å­¦è¿™ä¸ƒå¹´çš„ç ”å­¦æ—¶å…‰ï¼
										</p>
										<p>
											<!-- My research interests mainly revolve around Computer Vision, with
											the goal of developing general and generalizable vision systems. In the
											early few years ago, I focused on self-supervised learning, object detection,
											and person re-identification and their applications. Recently, I have been
											exploring Large Language and Multi-modal Models and their practical applications.
											I believe large models are one of the essential tools for facilitating
											people's interaction with modern vision systems and enhancing the quality
											of people's lives. If you have any questions would like to discuss with
											me, please feel free to contact me at drinkyyan@gmail.com. -->
											æˆ‘çš„ç ”ç©¶å…´è¶£ä¸»è¦å›´ç»•è®¡ç®—æœºè§†è§‰ï¼Œç›®æ ‡æ˜¯å¼€å‘é€šç”¨å’Œå¯æ¨å¹¿çš„è§†è§‰ç³»ç»Ÿã€‚åœ¨å…ˆå‰å‡ å¹´ï¼Œæˆ‘ä¸»è¦èšç„¦äºè‡ªç›‘ç£å­¦ä¹  (Self-supervised Learning)ã€ç›®æ ‡æ£€æµ‹
											(Object Detection) å’Œè¡Œäººé‡è¯†åˆ« (Person Re-identification) åŠå®ƒä»¬çš„åº”ç”¨ã€‚æœ€è¿‘ï¼Œæˆ‘åœ¨æŒç»­æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹
											(Large Language Model) ä»¥åŠå¤šæ¨¡æ€å¤§æ¨¡å‹ (Large Multi-modal Model) åŠå®ƒä»¬åœ¨å®é™…å·¥ç¨‹é¡¹ç›®å’Œäº§å“ä¸­çš„åº”ç”¨ã€‚æˆ‘ç›¸ä¿¡å¤§å‹æ¨¡å‹æ˜¯ä¿ƒè¿›äººç±»ä¸ç°ä»£å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿäº¤äº’å’Œæé«˜äººä»¬å·¥ä½œæ•ˆç‡ä»¥åŠç”Ÿæ´»è´¨é‡çš„é‡è¦å·¥å…·ä¹‹ä¸€ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æƒ³ä¸æˆ‘è®¨è®ºï¼Œè¯·éšæ—¶é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸æˆ‘è”ç³»ï¼šdrinkyyan@gmail.comã€‚
										</p>
										<p style="text-align:center">
											<a href="https://scholar.google.com.hk/citations?user=QMm29SwAAAAJ&hl=zh-CN">
												è°·æ­Œå­¦æœ¯ä¸»é¡µ
											</a>
											&nbsp;/&nbsp;&nbsp;
											<a href="https://github.com/yanjk3">
												Githubä¸ªäººä¸»é¡µ
											</a>
										</p>
									</td>
									<td style="padding:2.5%;width:40%;max-width:40%">
										<a href="images/drinky.png">
											<img style="width:100%;max-width:100%" alt="profile photo" src="images/drinky.png"
											class="hoverZoomLink">
										</a>
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											æœ€æ–°åŠ¨æ€
										</h2>
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/16/2024] 1 ç¯‡è®ºæ–‡è¢« ACM-MM 2024 æ¥æ”¶ä¸ºOral (3.97%)
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/01/2024] 2 ç¯‡è®ºæ–‡è¢« ECCV 2024 æ¥æ”¶
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[05/15/2024] 1 ç¯‡è®ºæ–‡è¢« KSEM 2024 æ¥æ”¶
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/15/2023] 1 ç¯‡è®ºæ–‡è¢« IEEE TPAMI æ¥æ”¶
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/13/2023] 1 ç¯‡è®ºæ–‡è¢« ICCV 2023 æ¥æ”¶
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/13/2023] 1 ç¯‡è®ºæ–‡è¢« ICME 2023 æ¥æ”¶ä¸º Oral
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[02/27/2023] 1 ç¯‡è®ºæ–‡è¢« CVPR 2023 æ¥æ”¶
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/29/2022] 1 ç¯‡è®ºæ–‡è¢« ICPR 2022 æ¥æ”¶ï¼ŒBest Industrial Research Paper Award Candidate
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											å·²å‘è¡¨/è¢«æ¥æ”¶çš„ç§‘ç ”è®ºæ–‡
										</h2>
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/loc4plan.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://openreview.net/forum?id=iYyxenA9gA">
													Loc4Plan: Locating Before Planning for Outdoor Vision and Language Navigation
												</a>
											</papertitle>
											<br>
											Huilin Tian, Jingke Meng, Wei-Shi Zheng, Yuan-Ming Li, 
											<strong>
												Junkai Yan
											</strong>
											, Yunong Zhang
											<br>
											<em>
												ACM-MM
											</em>
											, 2024, Oral
											<br>
											<br>
											è®©æœºå™¨äººåƒäººä¸€æ ·å­¦ä¼šå…ˆæ‰¾åˆ°è‡ªå·±çš„å®šä½ï¼Œå†çœ‹å¯¼èˆªæ€è€ƒè¯¥å¦‚ä½•èµ°ğŸ¤–
											<br></p>
										<div class="paper" id="8">
											<!-- <a href="https://github.com/yanjk3/CrossCL">
												[Code]
											</a> -->
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/dreamview.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2404.06119">
													DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation
												</a>
											</papertitle>
											<br>
											<strong>
												Junkai Yan*
											</strong>
											, Yipeng Gao*, Qize Yang, Xihan Wei, Xuansong Xie, Ancong Wu, Wei-Shi
											Zheng
											<br>
											<em>
												ECCV
											</em>
											, 2024
											<br>
											<br>
											ä½¿ç”¨å¤šè§†è§’æ–‡æœ¬æ¥ç²¾å‡†æ§åˆ¶æ‚¨æƒ³ç”Ÿæˆçš„3Då†…å®¹ğŸ¥³
											<br></p>
										<div class="paper" id="7">
											<a href="https://github.com/iSEE-Laboratory/DreamView">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/bpf.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2407.11499">
													Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection
												</a>
											</papertitle>
											<br>
											Qijie Mo, Shenghao Fu, Yipeng Gao,
											<strong>
												Junkai Yan
											</strong>
											, Ancong Wu, Wei-Shi Zheng
											<br>
											<em>
												ECCV
											</em>
											, 2024
											<br>
											<br>
											æ— éœ€è®°å¿†æ± ï¼Œå¸®æ‚¨ç¼“è§£å¢é‡ç›®æ ‡æ£€æµ‹ä¸­çš„é—å¿˜é—®é¢˜ğŸ§
											<br></p>
										<div class="paper" id="6">
											<a href="https://github.com/iSEE-Laboratory/BPF">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/ptma.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://link.springer.com/chapter/10.1007/978-981-97-5492-2_14">
													PTMA: Pre-trained Model Adaptation for Transfer Learning
												</a>
											</papertitle>
											<br>
											Xiao Li*,
											<strong>
												Junkai Yan*
											</strong>
											, Jianjian Jiang, Wei-Shi Zheng,
											<br>
											<em>
												KSEM
											</em>
											, 2024
											<br>
											<br>
											è·¨åŸŸä¸‹çš„è¿ç§»å­¦ä¹ ä¸å¥½ä½¿ï¼ŸPTMAæ¥å¸®æ‚¨ğŸ™Œ
											<br></p>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/versreid.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2403.11121">
													A Versatile Framework for Multi-scene Person Re-identification
												</a>
											</papertitle>
											<br>
											Wei-Shi Zheng*,
											<strong>
												Junkai Yan*
											</strong>
											, Yi-Xing Peng
											<br>
											<em>
												IEEE TPAMI
											</em>
											, 2024
											<br>
											<br>
											ä¸€é”…ç«¯ï¼šä½¿ç”¨ä¸€ä¸ªæ¨¡å‹å°±èƒ½è§£å†³å„ç§åœºæ™¯ä¸‹çš„è¡Œäººé‡è¯†åˆ«é—®é¢˜ğŸ¥‡
											<br></p>
										<div class="paper" id="4">
											<a href="https://github.com/iSEE-Laboratory/VersReID">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/asag.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2308.09242">
													ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive
													Sparse Anchor Generation
												</a>
											</papertitle>
											<br>
											Shenghao Fu,
											<strong>
												Junkai Yan
											</strong>
											, Yipeng Gao, Xiaohua Xie, Wei-Shi Zheng
											<br>
											<em>
												ICCV
											</em>
											, 2023
											<br>
											<br>
											æ‚¨çš„ç¨€ç–æ£€æµ‹å™¨ä¸ºä»€ä¹ˆéå¾—è¦
											<strong>
												6
											</strong>
											å±‚è§£ç å™¨å‘¢ğŸ¤”
											<br></p>
										<div class="paper" id="3">
											<a href="https://github.com/iSEE-Laboratory/ASAG">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/crosscl.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://ieeexplore.ieee.org/document/10219835">
													Self-supervised Cross-stage Regional Contrastive Learning for Object Detection
												</a>
											</papertitle>
											<br>
											<strong>
												Junkai Yan
											</strong>
											, Lingxiao Yang, Yipeng Gao, Wei-Shi Zheng
											<br>
											<em>
												ICME
											</em>
											, 2023, Oral
											<br>
											<br>
											æå‰é¢„ä¹ ä¸‹æ¸¸ä»»åŠ¡çš„å…ˆéªŒçŸ¥è¯†ï¼Œèƒ½æŒç»­æé«˜ç›®æ ‡æ£€æµ‹æ€§èƒ½ğŸ¤£
											<br></p>
										<div class="paper" id="2">
											<a href="https://github.com/yanjk3/CrossCL">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/asyfod.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="http://openaccess.thecvf.com/content/CVPR2023/html/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.html">
													AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive
													Object Detection
												</a>
											</papertitle>
											<br>
											Yipeng Gao*, Kun-Yu Lin*,
											<strong>
												Junkai Yan
											</strong>
											, Yaowei Wang, Wei-Shi Zheng
											<br>
											<em>
												CVPR
											</em>
											, 2023
											<br>
											<br>
											è·¨åŸŸç›®æ ‡æ£€æµ‹åˆç¼ºå°‘è®­ç»ƒæ ·æœ¬ï¼Ÿçœ‹çœ‹æˆ‘ä»¬çš„AsyFODğŸª„
											<br></p>
										<div class="paper" id="1">
											<a href="https://github.com/Hlings/AsyFOD">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/spacecl.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://ieeexplore.ieee.org/abstract/document/9956034">
													Space-correlated Contrastive Representation Learning with Multiple Instances
												</a>
											</papertitle>
											<br>
											Danming Song, Yipeng Gao,
											<strong>
												Junkai Yan
											</strong>
											, Wei Sun, Wei-Shi Zheng
											<br>
											<em>
												ICPR
											</em>
											, 2023
											<br>
											<br>
											ç¼“è§£å…¨å±€è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ è¿‡åˆ†ä¾èµ–å•å®ä¾‹å…ˆéªŒçš„é—®é¢˜ğŸŒŸ
											<br></p>
										<div class="paper" id="0">
											<a href="https://github.com/yanjk3/SpaceCL">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											å·²å…¬å¼€ä¸“åˆ©
										</h2>
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										è”åˆå¯¹æ¯”æŸå¤±å’Œé‡å»ºæŸå¤±çš„è‡ªç›‘ç£è§†è§‰è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼ˆCN116310667Aï¼‰
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										åŸºäºå¤šé‡å“ˆå¸Œçš„åˆ†å¸ƒå¼å­˜å‚¨ç´¢å¼•æ–¹æ³•åŠç³»ç»Ÿï¼ˆCN114416645Aï¼‰
									</td>
								</tr>
							</tbody>
						</table>
					</td>
				</tr>
			</tbody>
		</table>
	</body>

</html>
