<!DOCTYPE HTML>
<html lang="zh-CN">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>
			Junkai Yan
		</title>
		<meta name="author" content="Junkai Yan">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="stylesheet.css">
		<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>"></head>
	<body>
		<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tbody>
				<tr style="padding:0px">
					<td style="padding:0px">
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr style="padding:0px">
									<td style="padding:2.5%;width:63%;vertical-align:middle">
										<p class="name" style="text-align: center;">
											严俊楷 (Drinky)
										</p>
										<p>
											<!-- I obtained a Master's degree in Computer Technology from the School
											of Computer Science and Engineering at <a href="https://www.sysu.edu.cn/">Sun Yat-sen University (SYSU)</a> in June 2024, where I am advised by <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Prof. Wei-Shi Zheng</a>. Before achieving my Master degree, I also obtained my B.E. degree at SYSU. -->
											我于2024年6月毕业于
											<a href="https://www.sysu.edu.cn/">
												中山大学
											</a>
											计算机学院，获得工学硕士学位，专业为计算机技术。我的导师是
											<a href="https://www.isee-ai.cn/~zhwshi/">
												郑伟诗教授
											</a>
											。在我攻读硕士学位之前，我也在中山大学计算机学院获得了学士学位。我很感恩在中山大学这七年的研学时光！
										</p>
										<p>
											我的研究兴趣主要围绕计算机视觉，目标是开发通用和可推广的视觉系统。在先前几年，我主要聚焦于自监督学习 (Self-supervised Learning)、目标检测 (Object Detection) 和行人重识别 (Person Re-identification) 等相关视觉感知任务及它们在各种实际场景中的应用。如果您有任何问题想与我讨论，请随时通过以下方式与我联系：drinkyyan@gmail.com。
										</p>
										<p style="text-align:center">
											<a href="https://scholar.google.com.hk/citations?user=QMm29SwAAAAJ&hl=zh-CN">
												谷歌学术主页
											</a>
											&nbsp;/&nbsp;&nbsp;
											<a href="https://github.com/yanjk3">
												Github个人主页
											</a>
										</p>
									</td>
									<td style="padding:2.5%;width:40%;max-width:40%">
										<a href="images/drinky.png">
											<img style="width:100%;max-width:100%" alt="profile photo" src="images/drinky.png"
											class="hoverZoomLink">
										</a>
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											News 📰
										</h2>
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[09/26/2024] 1 篇论文被 NeurIPS 2024 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/16/2024] 1 篇论文被 ACM-MM 2024 接收为 Oral & Best Paper Nomination
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/01/2024] 2 篇论文被 ECCV 2024 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[05/15/2024] 1 篇论文被 KSEM 2024 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/15/2024] 1 篇论文被 IEEE TPAMI 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/13/2023] 1 篇论文被 ICCV 2023 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/13/2023] 1 篇论文被 ICME 2023 接收为 Oral
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[02/27/2023] 1 篇论文被 CVPR 2023 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/29/2022] 1 篇论文被 ICPR 2022 接收，Best Industrial Research Paper Award Candidate 
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											Publications 📚
										</h2>
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/frozendetr.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												Frozen-DETR: Enhancing DETR with Image Understanding from Frozen Foundation Models
											</papertitle>
											<br>
											Shenghao Fu, 
											<strong>
												Junkai Yan,
											</strong>
											Qize Yang, Xihan Wei, Xiaohua Xie, Wei-Shi Zheng
											<br>
											<em>
												NeurIPS
											</em>
											, 2024
											<br>
											<br>
											给您的检测器开上基础模型的外挂🚀
											<br></p>
										<div class="paper" id="8">
											<a href="https://arxiv.org/abs/2410.19635">
												[ArXiv]
											</a>
											<!-- <a href="https://github.com/iSEE-Laboratory/Frozen-DETR"></a> -->
												<!-- [Conference Paper] -->
											<!-- </a> -->
											<a href="https://github.com/iSEE-Laboratory/Frozen-DETR">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/loc4plan.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												Loc4Plan: Locating Before Planning for Outdoor Vision and Language Navigation
											</papertitle>
											<br>
											Huilin Tian*, Jingke Meng*, Wei-Shi Zheng, Yuan-Ming Li, 
											<strong>
												Junkai Yan,
											</strong>
											Yunong Zhang
											<br>
											<em>
												ACM-MM
											</em>
											, 2024, Oral, <strong>Best Paper Nomination</strong>
											<br>
											<br>
											让机器人像人一样学会先找到自己的定位，再看导航思考该如何走🤖
											<br></p>
										<div class="paper" id="8">
											<a href="https://arxiv.org/abs/2408.05090">
												[ArXiv]
											</a>
											<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681518">
												[Conference Paper]
											</a>
											<!-- <a href="https://github.com/iSEE-Laboratory/Frozen-DETR"> -->
												<!-- [Code] -->
											<!-- </a> -->
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/dreamview.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation
											</papertitle>
											<br>
											<strong>
												Junkai Yan*,
											</strong>
											Yipeng Gao*, Qize Yang, Xihan Wei, Xuansong Xie, Ancong Wu, Wei-Shi
											Zheng
											<br>
											<em>
												ECCV
											</em>
											, 2024
											<br>
											<br>
											使用多视角文本来精准控制您想生成的3D内容🥳
											<br></p>
										<div class="paper" id="7">
											<a href="https://arxiv.org/abs/2404.06119">
												[ArXiv]
											</a>
											<a href="https://link.springer.com/chapter/10.1007/978-3-031-72698-9_21">
												[Conference Paper]
											</a>
											<a href="https://github.com/iSEE-Laboratory/DreamView">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/bpf.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection
											</papertitle>
											<br>
											Qijie Mo, Shenghao Fu, Yipeng Gao,
											<strong>
												Junkai Yan,
											</strong>
											Ancong Wu, Wei-Shi Zheng
											<br>
											<em>
												ECCV
											</em>
											, 2024
											<br>
											<br>
											无需记忆池，帮您缓解增量目标检测中的遗忘问题🧐
											<br></p>
										<div class="paper" id="6">
											<a href="https://arxiv.org/abs/2407.11499">
												[ArXiv]
											</a>
											<a href="https://link.springer.com/chapter/10.1007/978-3-031-72640-8_26">
												[Conference Paper]
											</a>
											<a href="https://github.com/iSEE-Laboratory/BPF">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/ptma.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												PTMA: Pre-trained Model Adaptation for Transfer Learning
											</papertitle>
											<br>
											Xiao Li*,
											<strong>
												Junkai Yan*,
											</strong>
											Jianjian Jiang, Wei-Shi Zheng,
											<br>
											<em>
												KSEM
											</em>
											, 2024
											<br>
											<br>
											跨域下的迁移学习不好使？PTMA来帮您🙌
											<br></p>
										<div class="paper" id="5">
											<a href="https://link.springer.com/chapter/10.1007/978-981-97-5492-2_14">
												[Conference Paper]
											</a>
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/versreid.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												A Versatile Framework for Multi-scene Person Re-identification
											</papertitle>
											<br>
											Wei-Shi Zheng*,
											<strong>
												Junkai Yan*,
											</strong>
											Yi-Xing Peng
											<br>
											<em>
												IEEE TPAMI
											</em>
											, 2024
											<br>
											<br>
											一锅端：使用一个模型就能解决各种场景下的行人重识别问题🥇
											<br></p>
										<div class="paper" id="4">
											<a href="https://arxiv.org/abs/2403.11121">
												[ArXiv]
											</a>
											<a href="https://ieeexplore.ieee.org/abstract/document/10510353/">
												[Transaction Paper]
											</a>
											<a href="https://github.com/iSEE-Laboratory/VersReID">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/asag.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive
													Sparse Anchor Generation
											</papertitle>
											<br>
											Shenghao Fu,
											<strong>
												Junkai Yan,
											</strong>
											Yipeng Gao, Xiaohua Xie, Wei-Shi Zheng
											<br>
											<em>
												ICCV
											</em>
											, 2023
											<br>
											<br>
											您的稀疏检测器为什么非得要
											<strong>
												6
											</strong>
											层解码器呢🤔
											<br></p>
										<div class="paper" id="3">
											<a href="https://arxiv.org/abs/2308.09242">
												[ArXiv]
											</a>
											<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fu_ASAG_Building_Strong_One-Decoder-Layer_Sparse_Detectors_via_Adaptive_Sparse_Anchor_ICCV_2023_paper.html">
												[Conference Paper]
											</a>
											<a href="https://github.com/iSEE-Laboratory/ASAG">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/crosscl.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												Self-supervised Cross-stage Regional Contrastive Learning for Object Detection
											</papertitle>
											<br>
											<strong>
												Junkai Yan,
											</strong>
											Lingxiao Yang, Yipeng Gao, Wei-Shi Zheng
											<br>
											<em>
												ICME
											</em>
											, 2023, Oral
											<br>
											<br>
											提前预习下游任务的先验知识，能持续提高目标检测性能🤣
											<br></p>
										<div class="paper" id="2">
											<a href="https://ieeexplore.ieee.org/document/10219835">
												[Conference Paper]
											</a>
											<a href="https://github.com/yanjk3/CrossCL">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/asyfod.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection
											</papertitle>
											<br>
											Yipeng Gao*, Kun-Yu Lin*,
											<strong>
												Junkai Yan,
											</strong>
											Yaowei Wang, Wei-Shi Zheng
											<br>
											<em>
												CVPR
											</em>
											, 2023
											<br>
											<br>
											跨域目标检测又缺少训练样本？看看我们的AsyFOD🪄
											<br></p>
										<div class="paper" id="1">
											<a href="http://openaccess.thecvf.com/content/CVPR2023/html/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.html">
												[Conference Paper]
											</a>
											<a href="https://github.com/Hlings/AsyFOD">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/spacecl.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												Space-correlated Contrastive Representation Learning with Multiple Instances
											</papertitle>
											<br>
											Danming Song, Yipeng Gao,
											<strong>
												Junkai Yan,
											</strong>
											Wei Sun, Wei-Shi Zheng
											<br>
											<em>
												ICPR
											</em>
											, 2023
											<br>
											<br>
											缓解全局自监督对比学习过分依赖单实例先验的问题🌟
											<br></p>
										<div class="paper" id="0">
											<a href="https://ieeexplore.ieee.org/abstract/document/9956034">
												[Conference Paper]
											</a>
											<a href="https://github.com/yanjk3/SpaceCL">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											已公开专利
										</h2>
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										联合对比损失和重建损失的自监督视觉表征学习方法（CN116310667A）
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										基于多重哈希的分布式存储索引方法及系统（CN114416645A）
									</td>
								</tr>
							</tbody>
						</table>
					</td>
				</tr>
			</tbody>
		</table>
	</body>

</html>
