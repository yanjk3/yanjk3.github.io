<!DOCTYPE HTML>
<html lang="zh-CN">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>
			Junkai Yan
		</title>
		<meta name="author" content="Junkai Yan">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="stylesheet.css">
		<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>"></head>
	<body>
		<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tbody>
				<tr style="padding:0px">
					<td style="padding:0px">
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr style="padding:0px">
									<td style="padding:2.5%;width:63%;vertical-align:middle">
										<p class="name" style="text-align: center;">
											严俊楷 (Drinky)
										</p>
										<p>
											<!-- I obtained a Master's degree in Computer Technology from the School
											of Computer Science and Engineering at <a href="https://www.sysu.edu.cn/">Sun Yat-sen University (SYSU)</a> in June 2024, where I am advised by <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Prof. Wei-Shi Zheng</a>. Before achieving my Master degree, I also obtained my B.E. degree at SYSU. -->
											我于2024年6月毕业于
											<a href="https://www.sysu.edu.cn/">
												中山大学
											</a>
											计算机学院，获得工学硕士学位，专业为计算机技术。我的导师是
											<a href="https://www.isee-ai.cn/~zhwshi/">
												郑伟诗教授
											</a>
											。在我攻读硕士学位之前，我也在中山大学计算机学院获得了学士学位。我很感恩在中山大学这七年的研学时光！
										</p>
										<p>
											<!-- My research interests mainly revolve around Computer Vision, with
											the goal of developing general and generalizable vision systems. In the
											early few years ago, I focused on self-supervised learning, object detection,
											and person re-identification and their applications. Recently, I have been
											exploring Large Language and Multi-modal Models and their practical applications.
											I believe large models are one of the essential tools for facilitating
											people's interaction with modern vision systems and enhancing the quality
											of people's lives. If you have any questions would like to discuss with
											me, please feel free to contact me at drinkyyan@gmail.com. -->
											我的研究兴趣主要围绕计算机视觉，目标是开发通用和可推广的视觉系统。在先前几年，我主要聚焦于自监督学习 (Self-supervised Learning)、目标检测
											(Object Detection) 和行人重识别 (Person Re-identification) 及它们的应用。最近，我在持续探索大型语言模型
											(Large Language Model) 以及多模态大模型 (Large Multi-modal Model) 及它们在实际工程项目和产品中的应用。我相信大型模型是促进人类与现代多模态智能系统交互和提高人们工作效率以及生活质量的重要工具之一。如果您有任何问题想与我讨论，请随时通过以下方式与我联系：drinkyyan@gmail.com。
										</p>
										<p style="text-align:center">
											<a href="https://scholar.google.com.hk/citations?user=QMm29SwAAAAJ&hl=zh-CN">
												谷歌学术主页
											</a>
											&nbsp;/&nbsp;&nbsp;
											<a href="https://github.com/yanjk3">
												Github个人主页
											</a>
										</p>
									</td>
									<td style="padding:2.5%;width:40%;max-width:40%">
										<a href="images/drinky.png">
											<img style="width:100%;max-width:100%" alt="profile photo" src="images/drinky.png"
											class="hoverZoomLink">
										</a>
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											最新动态
										</h2>
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/16/2024] 1 篇论文被 ACM-MM 2024 接收为Oral (3.97%)
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/01/2024] 2 篇论文被 ECCV 2024 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[05/15/2024] 1 篇论文被 KSEM 2024 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/15/2023] 1 篇论文被 IEEE TPAMI 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[07/13/2023] 1 篇论文被 ICCV 2023 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/13/2023] 1 篇论文被 ICME 2023 接收为 Oral
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[02/27/2023] 1 篇论文被 CVPR 2023 接收
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										[03/29/2022] 1 篇论文被 ICPR 2022 接收，Best Industrial Research Paper Award Candidate
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											已发表/被接收的科研论文
										</h2>
									</td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/loc4plan.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://openreview.net/forum?id=iYyxenA9gA">
													Loc4Plan: Locating Before Planning for Outdoor Vision and Language Navigation
												</a>
											</papertitle>
											<br>
											Huilin Tian, Jingke Meng, Wei-Shi Zheng, Yuan-Ming Li, 
											<strong>
												Junkai Yan
											</strong>
											, Yunong Zhang
											<br>
											<em>
												ACM-MM
											</em>
											, 2024, Oral
											<br>
											<br>
											让机器人像人一样学会先找到自己的定位，再看导航思考该如何走🤖
											<br></p>
										<div class="paper" id="8">
											<!-- <a href="https://github.com/yanjk3/CrossCL">
												[Code]
											</a> -->
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/dreamview.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2404.06119">
													DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation
												</a>
											</papertitle>
											<br>
											<strong>
												Junkai Yan*
											</strong>
											, Yipeng Gao*, Qize Yang, Xihan Wei, Xuansong Xie, Ancong Wu, Wei-Shi
											Zheng
											<br>
											<em>
												ECCV
											</em>
											, 2024
											<br>
											<br>
											使用多视角文本来精准控制您想生成的3D内容🥳
											<br></p>
										<div class="paper" id="7">
											<a href="https://github.com/iSEE-Laboratory/DreamView">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/bpf.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2407.11499">
													Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection
												</a>
											</papertitle>
											<br>
											Qijie Mo, Shenghao Fu, Yipeng Gao,
											<strong>
												Junkai Yan
											</strong>
											, Ancong Wu, Wei-Shi Zheng
											<br>
											<em>
												ECCV
											</em>
											, 2024
											<br>
											<br>
											无需记忆池，帮您缓解增量目标检测中的遗忘问题🧐
											<br></p>
										<div class="paper" id="6">
											<a href="https://github.com/iSEE-Laboratory/BPF">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/ptma.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://link.springer.com/chapter/10.1007/978-981-97-5492-2_14">
													PTMA: Pre-trained Model Adaptation for Transfer Learning
												</a>
											</papertitle>
											<br>
											Xiao Li*,
											<strong>
												Junkai Yan*
											</strong>
											, Jianjian Jiang, Wei-Shi Zheng,
											<br>
											<em>
												KSEM
											</em>
											, 2024
											<br>
											<br>
											跨域下的迁移学习不好使？PTMA来帮您🙌
											<br></p>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/versreid.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2403.11121">
													A Versatile Framework for Multi-scene Person Re-identification
												</a>
											</papertitle>
											<br>
											Wei-Shi Zheng*,
											<strong>
												Junkai Yan*
											</strong>
											, Yi-Xing Peng
											<br>
											<em>
												IEEE TPAMI
											</em>
											, 2024
											<br>
											<br>
											一锅端：使用一个模型就能解决各种场景下的行人重识别问题🥇
											<br></p>
										<div class="paper" id="4">
											<a href="https://github.com/iSEE-Laboratory/VersReID">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/asag.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://arxiv.org/abs/2308.09242">
													ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive
													Sparse Anchor Generation
												</a>
											</papertitle>
											<br>
											Shenghao Fu,
											<strong>
												Junkai Yan
											</strong>
											, Yipeng Gao, Xiaohua Xie, Wei-Shi Zheng
											<br>
											<em>
												ICCV
											</em>
											, 2023
											<br>
											<br>
											您的稀疏检测器为什么非得要
											<strong>
												6
											</strong>
											层解码器呢🤔
											<br></p>
										<div class="paper" id="3">
											<a href="https://github.com/iSEE-Laboratory/ASAG">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr bgcolor="#ffffd0">
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/crosscl.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://ieeexplore.ieee.org/document/10219835">
													Self-supervised Cross-stage Regional Contrastive Learning for Object Detection
												</a>
											</papertitle>
											<br>
											<strong>
												Junkai Yan
											</strong>
											, Lingxiao Yang, Yipeng Gao, Wei-Shi Zheng
											<br>
											<em>
												ICME
											</em>
											, 2023, Oral
											<br>
											<br>
											提前预习下游任务的先验知识，能持续提高目标检测性能🤣
											<br></p>
										<div class="paper" id="2">
											<a href="https://github.com/yanjk3/CrossCL">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/asyfod.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="http://openaccess.thecvf.com/content/CVPR2023/html/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.html">
													AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive
													Object Detection
												</a>
											</papertitle>
											<br>
											Yipeng Gao*, Kun-Yu Lin*,
											<strong>
												Junkai Yan
											</strong>
											, Yaowei Wang, Wei-Shi Zheng
											<br>
											<em>
												CVPR
											</em>
											, 2023
											<br>
											<br>
											跨域目标检测又缺少训练样本？看看我们的AsyFOD🪄
											<br></p>
										<div class="paper" id="1">
											<a href="https://github.com/Hlings/AsyFOD">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
								<tr>
									<td style="padding:20px;width:35%;vertical-align:middle">
										<img src='images/spacecl.png' width="250"></div>
									</td>
									<td width="75%" valign="middle">
										<p>
											<papertitle>
												<a href="https://ieeexplore.ieee.org/abstract/document/9956034">
													Space-correlated Contrastive Representation Learning with Multiple Instances
												</a>
											</papertitle>
											<br>
											Danming Song, Yipeng Gao,
											<strong>
												Junkai Yan
											</strong>
											, Wei Sun, Wei-Shi Zheng
											<br>
											<em>
												ICPR
											</em>
											, 2023
											<br>
											<br>
											缓解全局自监督对比学习过分依赖单实例先验的问题🌟
											<br></p>
										<div class="paper" id="0">
											<a href="https://github.com/yanjk3/SpaceCL">
												[Code]
											</a>
										</div>
										<br></td>
								</tr>
							</tbody>
						</table>
						<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
							<tbody>
								<tr>
									<td style="padding:20px;width:100%;vertical-align:middle">
										<h2>
											已公开专利
										</h2>
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										联合对比损失和重建损失的自监督视觉表征学习方法（CN116310667A）
									</td>
								</tr>
								<tr>
									<td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
										基于多重哈希的分布式存储索引方法及系统（CN114416645A）
									</td>
								</tr>
							</tbody>
						</table>
					</td>
				</tr>
			</tbody>
		</table>
	</body>

</html>
